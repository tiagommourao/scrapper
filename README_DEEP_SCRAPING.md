# Scrapper com Deep Scraping ğŸ§¹ğŸ”

## âš¡ Nova Funcionalidade: Deep Scraping Recursivo

Esta versÃ£o estendida do **Scrapper** inclui uma poderosa funcionalidade de **Deep Scraping** que permite extrair conteÃºdo de forma recursiva de sites inteiros, controlando a profundidade de navegaÃ§Ã£o.

### ğŸ¯ O que Ã© Deep Scraping?

Diferentemente do scraping tradicional que analisa apenas uma pÃ¡gina, o Deep Scraping:

1. **Extrai conteÃºdo da URL base**
2. **Encontra todos os links na pÃ¡gina**
3. **Segue os links encontrados recursivamente**
4. **Organiza resultados hierarquicamente por nÃ­vel**
5. **Aplica filtros inteligentes** para evitar URLs problemÃ¡ticas

## ğŸš€ Quick Start

### 1. Setup RÃ¡pido
```bash
# Windows
./setup.bat

# Linux/Mac
chmod +x setup.sh
./setup.sh
```

### 2. Iniciar o Container
```bash
docker-compose up --build
```

### 3. Acessar a Interface
- **Deep Scraping UI**: http://localhost:3000/deep-scrape
- **API REST**: http://localhost:3000/api/deep-scrape
- **Docs**: http://localhost:3000/docs

## ğŸ›ï¸ Funcionalidades do Deep Scraping

### ParÃ¢metros de Controle
| ParÃ¢metro | DescriÃ§Ã£o | PadrÃ£o | Limites |
|-----------|-----------|---------|---------|
| **depth** | Profundidade de recursÃ£o | 3 | 1-10 |
| **max-urls-per-level** | URLs mÃ¡ximas por nÃ­vel | 10 | 1-50 |
| **same-domain-only** | Restringir ao mesmo domÃ­nio | true | boolean |
| **delay-between-requests** | Delay entre requisiÃ§Ãµes (seg) | 1.0 | 0.1-10.0 |
| **exclude-patterns** | PadrÃµes de URL para excluir | null | string |

### Interface Web Intuitiva
- âœ… **FormulÃ¡rio dedicado** com controles especÃ­ficos
- âœ… **ConfiguraÃ§Ã£o visual** de parÃ¢metros
- âœ… **VisualizaÃ§Ã£o hierÃ¡rquica** dos resultados
- âœ… **Resumo estatÃ­stico** de pÃ¡ginas processadas

### API REST Completa
```bash
# Exemplo bÃ¡sico
curl "http://localhost:3000/api/deep-scrape?url=https://example.com&depth=3"

# Exemplo avanÃ§ado
curl "http://localhost:3000/api/deep-scrape?url=https://docs.example.com&depth=4&max-urls-per-level=15&same-domain-only=true&exclude-patterns=/admin,/login"
```

## ğŸ“Š Estrutura de Resposta

```json
{
  "id": "abc123",
  "base_url": "https://example.com",
  "domain": "example.com",
  "total_pages": 25,
  "levels": [
    {
      "level": 0,
      "pages": [
        {
          "url": "https://example.com",
          "title": "Home Page",
          "content": "<p>ConteÃºdo extraÃ­do...</p>",
          "textContent": "Texto limpo...",
          "meta": {...}
        }
      ]
    },
    {
      "level": 1,
      "pages": [...]
    }
  ]
}
```

## ğŸ”’ SeguranÃ§a e Filtros

### Filtros AutomÃ¡ticos
- **URLs problemÃ¡ticas**: `/login`, `/admin`, `/logout`
- **Arquivos binÃ¡rios**: `.pdf`, `.zip`, `.exe`
- **Protocolos especiais**: `mailto:`, `tel:`, `javascript:`
- **APIs e feeds**: `/api/`, `/rss/`, `/feed/`

### Rate Limiting Inteligente
- **Delay configurÃ¡vel** entre requisiÃ§Ãµes
- **Respeito aos servidores** com limites sensatos
- **Controle de concorrÃªncia** otimizado

## ğŸ¯ Casos de Uso

### 1. DocumentaÃ§Ã£o TÃ©cnica
```bash
# Extrair documentaÃ§Ã£o completa
curl "localhost:3000/api/deep-scrape?url=https://docs.python.org&depth=4&same-domain-only=true"
```

### 2. Portal de NotÃ­cias
```bash
# Scraping de artigos recentes
curl "localhost:3000/api/deep-scrape?url=https://techcrunch.com&depth=2&max-urls-per-level=20"
```

### 3. Site Corporativo
```bash
# AnÃ¡lise completa de conteÃºdo
curl "localhost:3000/api/deep-scrape?url=https://company.com&depth=3&exclude-patterns=/careers,/contact"
```

## ğŸ› ï¸ Tecnologias e Arquitetura

### Backend Robusto
- **Python + FastAPI** para performance
- **Playwright** para browser automation
- **Readability.js** para extraÃ§Ã£o de conteÃºdo
- **Algoritmo BFS** para navegaÃ§Ã£o eficiente

### Frontend Moderno
- **Interface responsiva** com Pico CSS
- **Controles dinÃ¢micos** para configuraÃ§Ã£o
- **VisualizaÃ§Ã£o hierÃ¡rquica** dos resultados
- **JavaScript otimizado** para UX fluÃ­da

### Infraestrutura
- **Docker + Docker Compose** para deploy
- **Cache inteligente** para otimizaÃ§Ã£o
- **Logs detalhados** para monitoramento
- **Health checks** para confiabilidade

## âš¡ Performance e OtimizaÃ§Ãµes

### Cache AvanÃ§ado
- **Evita URLs duplicadas** automaticamente
- **Cache de resultados** para consultas repetidas
- **Limpeza automÃ¡tica** de arquivos antigos

### Controle de Recursos
- **SemÃ¡foros** para concorrÃªncia controlada
- **Timeouts configurÃ¡veis** para stability
- **Cleanup automÃ¡tico** de contextos de browser

## ğŸ“ˆ Monitoramento

### Logs Estruturados
```bash
# Acompanhar deep scraping em tempo real
docker-compose logs -f scrapper | grep "deep-scrape"
```

### MÃ©tricas DisponÃ­veis
- Total de pÃ¡ginas processadas
- Tempo de execuÃ§Ã£o por nÃ­vel
- URLs filtradas e ignoradas
- Erros por profundidade

## ğŸ”„ Comparativo de Funcionalidades

| Funcionalidade | Article | Links | **Deep Scrape** |
|----------------|---------|-------|-----------------|
| PÃ¡ginas processadas | 1 | 1 | 1-500+ |
| Estrutura de dados | Simples | Lista | **HierÃ¡rquica** |
| Controle de profundidade | âŒ | âŒ | **âœ…** |
| NavegaÃ§Ã£o recursiva | âŒ | âŒ | **âœ…** |
| Filtros inteligentes | âŒ | BÃ¡sico | **AvanÃ§ados** |
| Rate limiting | âŒ | âŒ | **âœ…** |

## ğŸ¤ Contribuindo

### Melhorias Futuras
- [ ] **Processamento paralelo** por nÃ­vel
- [ ] **Base de dados** para resultados grandes
- [ ] **API de progresso** em tempo real
- [ ] **Webhooks** para notificaÃ§Ãµes
- [ ] **ML para relevÃ¢ncia** de conteÃºdo

### Como Contribuir
1. Fork o repositÃ³rio
2. Crie uma branch para sua feature
3. Implemente os testes
4. Submeta um Pull Request

## ğŸ“ Suporte

- **Issues**: [GitHub Issues](https://github.com/amerkurev/scrapper/issues)
- **DocumentaÃ§Ã£o**: `/docs/DEEP_SCRAPING.md`
- **API Docs**: http://localhost:3000/docs

---

**ğŸ‰ Agora vocÃª tem o poder do Deep Scraping em suas mÃ£os!**

Extraia conteÃºdo de sites inteiros de forma inteligente, respeitosa e eficiente. ğŸš€ 