# Deep Scraping - Scrapping Recursivo

## üéØ Vis√£o Geral

A funcionalidade **Deep Scraping** do Scrapper permite extrair conte√∫do de forma recursiva de um site inteiro, controlando a profundidade de navega√ß√£o. Diferentemente do scraping tradicional que analisa apenas uma p√°gina, o Deep Scraping:

1. **Extrai conte√∫do da URL base**
2. **Encontra todos os links na p√°gina**
3. **Segue os links encontrados**
4. **Repete o processo recursivamente** at√© a profundidade especificada

## üöÄ Como Usar

### Interface Web
Acesse `http://localhost:3000/deep-scrape` e configure:

- **URL Base**: A URL raiz para iniciar o scraping
- **Profundidade**: N√≠veis de recurs√£o (1-10)
- **URLs por N√≠vel**: Limite de URLs processadas por n√≠vel
- **Mesmo Dom√≠nio**: Restringir ao dom√≠nio original
- **Delay**: Intervalo entre requisi√ß√µes (segundos)
- **Padr√µes de Exclus√£o**: URLs a serem ignoradas

### API REST

```bash
curl -X GET "http://localhost:3000/api/deep-scrape?url=https://example.com&depth=3&max-urls-per-level=10"
```

## üìã Par√¢metros

### Par√¢metros Espec√≠ficos do Deep Scraping

| Par√¢metro | Descri√ß√£o | Padr√£o | Limites |
|-----------|-----------|---------|---------|
| `depth` | Profundidade m√°xima de recurs√£o | 3 | 1-10 |
| `max-urls-per-level` | M√°ximo de URLs por n√≠vel | 10 | 1-50 |
| `same-domain-only` | Restringir ao mesmo dom√≠nio | true | boolean |
| `delay-between-requests` | Delay entre requisi√ß√µes (segundos) | 1.0 | 0.1-10.0 |
| `exclude-patterns` | Padr√µes de URL para excluir | null | string |

### Par√¢metros Herdados
O Deep Scraping tamb√©m aceita todos os par√¢metros do scraping tradicional:
- Configura√ß√µes de browser (`device`, `timeout`, `user-agent`, etc.)
- Configura√ß√µes de proxy (`proxy-server`, `proxy-username`, etc.)
- Configura√ß√µes do Readability (`char-threshold`, `max-elems-to-parse`, etc.)
- Configura√ß√µes gerais (`cache`, `screenshot`, `full-content`, etc.)

## üìä Estrutura de Resposta

```json
{
  "id": "abc123",
  "base_url": "https://example.com",
  "domain": "example.com",
  "date": "2024-01-15T10:30:00Z",
  "total_pages": 25,
  "query": {...},
  "resultUri": "http://localhost:3000/result/abc123",
  "screenshotUri": "http://localhost:3000/screenshot/abc123",
  "levels": [
    {
      "level": 0,
      "pages": [
        {
          "url": "https://example.com",
          "title": "Home Page",
          "content": "<p>Conte√∫do extra√≠do...</p>",
          "textContent": "Conte√∫do em texto...",
          "level": 0,
          "parent_index": -1,
          "meta": {...}
        }
      ]
    },
    {
      "level": 1,
      "pages": [
        {
          "url": "https://example.com/about",
          "title": "Sobre N√≥s",
          "content": "<p>Mais conte√∫do...</p>",
          "level": 1,
          "parent_index": 0,
          "meta": {...}
        }
      ]
    }
  ]
}
```

## üîí Filtros de Seguran√ßa

O sistema inclui filtros autom√°ticos para evitar URLs problem√°ticas:

### URLs Exclu√≠das Automaticamente
- `/login`, `/logout`, `/register`, `/signup`, `/admin`
- Arquivos: `.pdf`, `.doc`, `.docx`, `.zip`, `.exe`, `.dmg`
- Protocolos especiais: `mailto:`, `tel:`, `javascript:`
- Feeds: `/feed`, `/rss`, `/api/`, `/ajax/`
- Fragmentos: URLs com `#` apenas

### Padr√µes Personalizados
Use o par√¢metro `exclude-patterns` para adicionar seus pr√≥prios filtros:
```
exclude-patterns=/admin,/login,/private,/download
```

## ‚ö° Otimiza√ß√µes de Performance

### Rate Limiting
- **Delay configur√°vel** entre requisi√ß√µes
- **Limite de URLs por n√≠vel** para evitar sobrecarga
- **Limite de profundidade** para controlar recursos

### Cache Inteligente
- **Evita URLs duplicadas** automaticamente
- **Cache de resultados** para requisi√ß√µes repetidas
- **Reutiliza√ß√£o** de contextos de browser

### Processamento Paralelo
- **Sem√°foros** para controlar concorr√™ncia
- **Contexts isolados** para cada p√°gina
- **Cleanup autom√°tico** de recursos

## üéØ Casos de Uso

### 1. Documenta√ß√£o T√©cnica
```bash
# Extrair toda documenta√ß√£o de um projeto
curl "localhost:3000/api/deep-scrape?url=https://docs.example.com&depth=4&same-domain-only=true"
```

### 2. Portal de Not√≠cias
```bash
# Scraping de artigos recentes
curl "localhost:3000/api/deep-scrape?url=https://news.example.com&depth=2&max-urls-per-level=20"
```

### 3. Site Corporativo
```bash
# An√°lise completa de site
curl "localhost:3000/api/deep-scrape?url=https://company.com&depth=3&exclude-patterns=/careers,/contact"
```

## üõ°Ô∏è Considera√ß√µes √âticas

### Respeito aos Servidores
- **Delay obrigat√≥rio** entre requisi√ß√µes (m√≠nimo 0.1s)
- **Limites sensatos** de URLs e profundidade
- **User-Agent identific√°vel** nas requisi√ß√µes

### Conformidade Legal
- **Respeite robots.txt** manualmente
- **Verifique termos de uso** dos sites
- **Use apenas para fins leg√≠timos**

### Boas Pr√°ticas
- **Teste com profundidade baixa** primeiro
- **Monitor logs** para detectar problemas
- **Configure delays adequados** para cada site

## üîß Troubleshooting

### Erro: "Muitas URLs encontradas"
- Reduza `max-urls-per-level`
- Adicione `exclude-patterns` mais espec√≠ficos
- Diminua a `depth`

### Erro: "Timeout na requisi√ß√£o"
- Aumente `timeout` nos par√¢metros de browser
- Verifique conectividade com o site
- Reduza `max-urls-per-level`

### Performance Lenta
- Aumente `delay-between-requests`
- Reduza `depth` e `max-urls-per-level`
- Use `same-domain-only=true`

## üìà Monitoramento

### Logs Importantes
```bash
# Acompanhar progresso do deep scraping
docker-compose logs -f scrapper | grep "deep-scrape"
```

### M√©tricas de Performance
- **Total de p√°ginas** processadas
- **Tempo total** de execu√ß√£o
- **URLs ignoradas** por filtros
- **Erros por n√≠vel** de profundidade

## üîÑ Atualiza√ß√µes Futuras

### Funcionalidades Planejadas
- [ ] **Scraping paralelo** por n√≠vel
- [ ] **Base de dados** para resultados grandes
- [ ] **API de status** para acompanhar progresso
- [ ] **Webhooks** para notifica√ß√µes
- [ ] **Filtros ML** para relev√¢ncia de conte√∫do

### Melhorias de Performance
- [ ] **Redis** para cache distribu√≠do
- [ ] **Queue system** para processamento ass√≠ncrono
- [ ] **CDN** para screenshots e assets
- [ ] **Compress√£o** de resultados grandes 